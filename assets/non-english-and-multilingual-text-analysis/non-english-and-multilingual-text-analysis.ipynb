{
 "cells": [
 {
    "cell_type": "markdown",
    "metadata": {
     "colab_type": "text",
     "id": "view-in-github"
    },
    "source": [
     "<a href=\"https://colab.research.google.com/github/programminghistorian/ph-submissions/blob/gh-pages/assets/non-english-and-multilingual-text-analysis/non-english-and-multilingual-text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis for Non-English and Multilingual Texts: Using NLTK, spaCy, and Stanza to Process a Text in French and Russian Using Part-of-Speech Tagging, Language Detection, and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Goals\n",
    "## Preparation\n",
    "## Basics of Text Analysis and Working with Non-English and Multilingual Text\n",
    "## Tools We’ll Cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code and Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "war_and_peace = \"\"\"\n",
    "— Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\n",
    "\n",
    "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:\n",
    "\n",
    "«Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».\n",
    "\n",
    "— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
    "\n",
    "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
    "\n",
    "— Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"war_and_peace_excerpt.txt\") as file:\n",
    "    war_and_peace = file.read()\n",
    "    print(war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_war_and_peace = war_and_peace.replace(\"\\n\", \" \")\n",
    "print(cleaned_war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# first, let's install the 'punkt' resources required to use the tokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# then we import the sent_tokenize method and apply it to our war_and_peace variable\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk_sent_tokenized = sent_tokenize(cleaned_war_and_peace)\n",
    "# if you were going to specify a language, the following syntax would be used: nltk_sent_tokenized = sent_tokenize(war_and_peace, language=\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# printing the Russian sentence at index 5 in our list of sentence\n",
    "rus_sent = nltk_sent_tokenized[5]\n",
    "print('Russian: ' + rus_sent)\n",
    "\n",
    "# printing the French sentence at index 2\n",
    "fre_sent = nltk_sent_tokenized[13]\n",
    "print('French: ' + fre_sent)\n",
    "\n",
    "# printing the sentence in both French and Russian at index 4\n",
    "multi_sent = nltk_sent_tokenized[4]\n",
    "print('Multilang: ' + multi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# printing each sentence in our list\n",
    "for sent in nltk_sent_tokenized:\n",
    "  print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# downloading our multilingual sentence tokenizer\n",
    "python -m spacy download xx_sent_ud_sm\n",
    "\n",
    "# loading the multilingual sentence tokenizer we just downloaded\n",
    "nlp = spacy.load(\"xx_sent_ud_sm\")\n",
    "# applying the spaCy model to our text variable\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# assigning the tokenized sentences to a list so it's easier for us to manipulate them later\n",
    "spacy_sentences = list(doc.sents)\n",
    "\n",
    "# printing the sentences to our console\n",
    "print(spacy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# concatenating the Russian sentence and its language label\n",
    "spacy_rus_sent = str(spacy_sentences[5])\n",
    "print('Russian: ' + spacy_rus_sent)\n",
    "\n",
    "# concatenating the French sentence and its language label\n",
    "spacy_fre_sent = str(spacy_sentences[13])\n",
    "print('French: ' + spacy_fre_sent)\n",
    "\n",
    "# concatenating the French and Russian sentence and its label\n",
    "spacy_multi_sent = str(spacy_sentences[4])\n",
    "print('Multilang: ' + spacy_multi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# setting up our tokenizer pipeline\n",
    "nlp = MultilingualPipeline(processors='tokenize')\n",
    "\n",
    "# applying the pipeline to our text\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# printing all sentences to see how they tokenized\n",
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# creating an empty list to append our sentences to\n",
    "stanza_sentences = []\n",
    "\n",
    "# iterating through the sentence tokens created by the tokenizer pipeline and appending to the list\n",
    "for sentence in doc.sentences:\n",
    "  stanza_sentences.append(sentence.text)\n",
    "\n",
    "# printing our sentence that is only in Russian\n",
    "stanza_rus_sent = str(stanza_sentences[5])\n",
    "print('Russian: ' + stanza_rus_sent)\n",
    "\n",
    "# printing our sentence that is only in French\n",
    "stanza_fre_sent = str(stanza_sentences[12])\n",
    "print('French: ' + stanza_fre_sent)\n",
    "\n",
    "# printing our sentence in both French and Russian\n",
    "stanza_multi_sent = str(stanza_sentences[4])\n",
    "print('Multilang: ' + stanza_multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically Detecting Different Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# downloading an NLTK corpus reader required by the TextCat module\n",
    "nltk.download('crubadan')\n",
    "\n",
    "# loading the TextCat package and applying it to each of our sentences\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "rus_estimate = tcat.guess_language(rus_sent)\n",
    "fre_estimate = tcat.guess_language(fre_sent)\n",
    "multi_estimate = tcat.guess_language(multi_sent)\n",
    "\n",
    "# printing the results\n",
    "print('Russian estimate: ' + rus_estimate)\n",
    "print('French estimate: ' + fre_estimate)\n",
    "print('Multilingual estimate: ' + multi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# first, we install the spacy_langdetect package from the Python Package Index\n",
    "pip install spacy_langdetect\n",
    "\n",
    "# then we import it and use it to detect our languages\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# setting up our language detector to work with spaCy\n",
    "# def get_lang_detector(nlp, name):\n",
    "#     return LanguageDetector()\n",
    "\n",
    "# setting up our pipeline\n",
    "Language.factory(\"language_detector\")\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "# running the language detection on each sentence and printing the results\n",
    "rus_doc = nlp(spacy_rus_sent)\n",
    "print(rus_doc._.language)\n",
    "\n",
    "fre_doc = nlp(spacy_fre_sent)\n",
    "print(fre_doc._.language)\n",
    "\n",
    "multi_doc = nlp(spacy_multi_sent)\n",
    "print(multi_doc._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing our models required for language detection\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "\n",
    "# setting up our pipeline\n",
    "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\")\n",
    "\n",
    "# specifying which sentences to run the detection on, then running the detection code\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "docs = [Document([], text=text) for text in docs]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the text of each sentence alongside the language estimates\n",
    "print(\"\\n\".join(f\"{doc.text}\\t{doc.lang}\" for doc in docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# first, we split the sentence into its component words using the wordpunct_tokenize module\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokenized_sent = wordpunct_tokenize(multi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing the regex package so we can use a regular expression\n",
    "import regex\n",
    "# importing the string package to detect punctuation\n",
    "from string import punctuation\n",
    "\n",
    "# setting empty lists we will later populate with our words\n",
    "cyrillic_words = []\n",
    "latin_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for word in tokenized_sent:\n",
    "  if word in punctuation:\n",
    "    continue\n",
    "  else:\n",
    "    if regex.search(r'\\p{IsCyrillic}', word):\n",
    "      cyrillic_words.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# joining the lists into a string, with each word separated by a space (' ')\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# now we use TextCat again to detect their languages\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "multi_estimate_1 = tcat.guess_language(cyrillic_only_list)\n",
    "multi_estimate_2 = tcat.guess_language(latin_only_list)\n",
    "\n",
    "# printing our estimates\n",
    "print('Cyrillic estimate: ' + multi_estimate_1)\n",
    "print('Latin estimate: ' + multi_estimate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# downloading our Russian model from spaCy\n",
    "python -m spacy download ru_core_news_sm\n",
    "\n",
    "\n",
    "# loading the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_rus_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# downloading our French model from spaCy\n",
    "python -m spacy download fr_core_news_sm\n",
    "\n",
    "\n",
    "# loading the corpus\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_fre_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# creating our blank lists to append to later\n",
    "cyrillic_words_punct = []\n",
    "latin_words_punct = []\n",
    "\n",
    "# initializing a blank string to keep track of the last list we appended to\n",
    "last_appended_list = ''\n",
    "\n",
    "# iterating through our words and appending based on whether a Cyrillic character was detected\n",
    "for word in tokenized_sent:\n",
    "  if regex.search(r'\\p{IsCyrillic}', word):\n",
    "    cyrillic_words_punct.append(word)\n",
    "    # updating our string to track the list we appended a word to\n",
    "    last_appended_list = 'cyr'\n",
    "  else:\n",
    "    # handling punctuation by appending it to our most recently used list\n",
    "    if word in punctuation:\n",
    "        if last_appended_list == 'cyr':\n",
    "            cyrillic_words_punct.append(word)\n",
    "        elif last_appended_list == 'lat':\n",
    "            latin_words_punct.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "        last_appended_list = 'lat'\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# joining the lists to strings\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# using our regular expression to remove extra whitespace before the punctuation marks\n",
    "cyr_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', cyrillic_only_list)\n",
    "lat_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', latin_only_list)\n",
    "\n",
    "# checking the results of the regular expression above\n",
    "print(cyr_no_extra_space)\n",
    "print(lat_no_extra_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# and doing the same with our French sentence\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as Russian ('ru')\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')\n",
    "doc = nlp(stanza_rus_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as French ('fr')\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos')\n",
    "doc = nlp(stanza_fre_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# imports so we can use Stanza's MultilingualPipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# running the multilingual pipeline on our French, Russian, and multilingual sentences simultaneously\n",
    "nlp = MultilingualPipeline(processors='tokenize,pos')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the results\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading and applying our Russian model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(spacy_rus_sent)\n",
    "\n",
    "\n",
    "# printing each word alongside its lemma\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)\n",
    "\n",
    "# loading and applying our French model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(spacy_fre_sent)\n",
    "\n",
    "# again printing each word alongside its lemma\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# imports so we can run the multilingual pipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# adding the 'lemma' processor to the pipeline and running it on our sentences\n",
    "nlp = MultilingualPipeline(processors='tokenize,lemma')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlped_docs = nlp(docs)\n",
    "\n",
    "# iterating through each sentence's words and printing the lemmas\n",
    "for doc in nlped_docs:\n",
    "  lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]\n",
    "  print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
